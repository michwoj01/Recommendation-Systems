{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d18a343-c53e-4e64-8316-f5c1167437c4",
   "metadata": {},
   "source": [
    "# Laboratorium 6 - rekomendacje oparte na grafach wiedzy\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge\n",
    "   * na potrzeby drugiej części laboratorium (czyli testowego treningu), na Teamsach macie dostępny podzbiór danych, `a_few_playlists_dataset` - nie wystarczy on jednak do wykonania trzeciej części (i tym samym do oddania laboratorium)\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab6`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install numpy pandas pykeen tqdm seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a763f-eb56-4085-ac43-6d7a1a0cb520",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e62e94d-9961-4d5e-a264-745b5177ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/Rekomendacyjne/recsyslab1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from pykeen.models import TransE\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.predict import predict_target\n",
    "from pykeen.triples import TriplesFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67368f56-c5ee-45cb-90f7-235d610c40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # konfiguracja datasetu Spotify\n",
    "# PATH = 'spotify_million_playlist_dataset/data'\n",
    "# SAMPLING_RATIO = 1.0\n",
    "# FILENAMES = random.sample([f'mpd.slice.{1000*i}-{1000*i+999}.json' for i in range(1000)], int(SAMPLING_RATIO*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4d3db7-5e55-4a11-a45e-79356fd75e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jesli uzywasz datasetu pobranego z Teamsow, uzyj tej komorki zamiast powyzszej\n",
    "# UWAGA - do oddania laboratorium konieczne jest uzycie oficjalnego datasetu Spotify\n",
    "PATH = 'a_few_playlists_dataset'\n",
    "SAMPLING_RATIO = 0.01\n",
    "with open(f'{PATH}/filenames.txt') as fn:\n",
    "    FILENAMES = fn.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7743626d-5017-4443-809c-f3146bf82298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcje do parsowania playlist\n",
    "def get_id(uri):\n",
    "    return uri.split(':')[-1]\n",
    "\n",
    "def parse_playlist(playlist):\n",
    "    name = playlist['name']\n",
    "    tracks = [get_id(t['track_uri']) for t in playlist['tracks']]\n",
    "    tracks_to_artists = {(get_id(t['track_uri']), get_id(t['artist_uri'])) for t in playlist['tracks']}\n",
    "    tracks_to_albums = {(get_id(t['track_uri']), get_id(t['album_uri'])) for t in playlist['tracks']}\n",
    "    albums_to_artists = {(get_id(t['album_uri']), get_id(t['artist_uri'])) for t in playlist['tracks']}\n",
    "    return name, tracks, tracks_to_artists, tracks_to_albums, albums_to_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475e2848-3c30-4173-98dc-272fb86e4690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got: 10,000 playlists; 175,992 tracks; 84,206 albums; 36,844 artists\n"
     ]
    }
   ],
   "source": [
    "# parsing\n",
    "playlists = []\n",
    "tracks = set()\n",
    "tracks_to_artists = set()\n",
    "tracks_to_albums = set()\n",
    "albums_to_artists = set()\n",
    "\n",
    "for filename in tqdm(FILENAMES):\n",
    "    with open(f'{PATH}/{filename}') as mpd_chunk:\n",
    "        for playlist in json.loads(mpd_chunk.read())['playlists']:\n",
    "            a, b, c, d, e = parse_playlist(playlist)\n",
    "            playlists.append(b)\n",
    "            tracks.update(set(b))\n",
    "            tracks_to_artists.update(c)\n",
    "            tracks_to_albums.update(d)\n",
    "            albums_to_artists.update(e)\n",
    "\n",
    "print(f'Got: {len(playlists):,} playlists; {len(tracks):,} tracks; {len({x[1] for x in tracks_to_albums}):,} albums; {len({x[1] for x in tracks_to_artists}):,} artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a6478c-9602-4e40-ae2f-8eeea8a665ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15386.47it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 24903.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 9,000; test dataset: 1,000\n"
     ]
    }
   ],
   "source": [
    "# w zbiorze testowym chcemy tylko te playlisty, ktorych wszystkie piosenki wystapia takze choc raz w zbiorze treningowym\n",
    "tracks_counter = {}\n",
    "for p in tqdm(playlists):\n",
    "    for t in p:\n",
    "        if t in tracks_counter:\n",
    "            tracks_counter[t] += 1\n",
    "        else:\n",
    "            tracks_counter[t] = 1\n",
    "\n",
    "playlists_ids_with_only_non_unique_tracks = []\n",
    "for i in tqdm(range(len((playlists)))):\n",
    "    p = playlists[i]\n",
    "    if all([tracks_counter[t] > 1 for t in p]):\n",
    "        playlists_ids_with_only_non_unique_tracks.append(i)\n",
    "\n",
    "# zbior testowy to 1/10 wszystkich playlist - czyli 100k, jesli nie używamy samplingu\n",
    "test_playlist_ids = random.sample(playlists_ids_with_only_non_unique_tracks, int(SAMPLING_RATIO*100_000))\n",
    "# zbior treningowy to cala reszta playlist - jest ich duzo, wiec sprobujmy to zrobic wydajnie\n",
    "test_ids_sorted = sorted(test_playlist_ids)\n",
    "test_i = 0\n",
    "train_playlist_ids = []\n",
    "i = 0\n",
    "while i < len(playlists):\n",
    "    if test_i < len(test_ids_sorted) and test_ids_sorted[test_i] == i:\n",
    "        test_i += 1\n",
    "    else:\n",
    "        train_playlist_ids.append(i)\n",
    "    i += 1\n",
    "\n",
    "train_playlists = [playlists[i] for i in train_playlist_ids]\n",
    "test_playlists = [playlists[i] for i in test_playlist_ids]\n",
    "\n",
    "print(f'train dataset: {len(train_playlists):,}; test dataset: {len(test_playlists):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480cce3-90df-4f6e-b781-0c30f6c18c31",
   "metadata": {},
   "source": [
    "## Część 2. - budowa i ewaluacja modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bb789-088e-4d5b-82ac-15116fb31216",
   "metadata": {},
   "source": [
    "### Relacje istniejące w naszym datasecie:\n",
    "![poglądowy obrazek relacji w datasecie](relations.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83962cc3-54fd-438d-a7c7-b8a2f3c1b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/175992 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175992/175992 [00:00<00:00, 525948.10it/s]\n",
      "100%|██████████| 175992/175992 [00:00<00:00, 639783.76it/s]\n",
      "100%|██████████| 88378/88378 [00:00<00:00, 707030.74it/s]\n",
      "100%|██████████| 9000/9000 [00:00<00:00, 17588.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 4 relations with total of 1,065,707 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# budowanie zbioru relacji\n",
    "# mozesz usunac czesc wpisow z listy `relations`\n",
    "relations = ['follows', 'authored_by', 'in_album', 'authored']\n",
    "triples = []\n",
    "\n",
    "# relacje piosenka -> autor\n",
    "if 'authored_by' in relations:\n",
    "    for track, artist in tqdm(tracks_to_artists):\n",
    "        triples.append((track, 'authored_by', artist))\n",
    "\n",
    "# relacje piosenka -> artysta\n",
    "if 'in_album' in relations:\n",
    "    for track, album in tqdm(tracks_to_albums):\n",
    "        triples.append((track, 'in_album', album))\n",
    "\n",
    "# relacje artysta -> album\n",
    "if 'authored' in relations:\n",
    "    for album, artist in tqdm(albums_to_artists):\n",
    "        triples.append((artist, 'authored', album))\n",
    "\n",
    "# relacje piosenka -> piosenka\n",
    "if 'follows' in relations:\n",
    "    for playlist in tqdm(train_playlists):\n",
    "        for i in range(len(playlist)-1):\n",
    "            triples.append((playlist[i], 'follows', playlist[i+1]))\n",
    "\n",
    "num_entities = len(triples)\n",
    "num_relations = len(relations)\n",
    "\n",
    "print(f'Got {num_relations} relations with total of {num_entities:,} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a2042a-3165-4177-af82-2cb1586c8eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=2027330857\n",
      "No random seed is specified. Setting to 1473018507.\n",
      "No cuda devices were available. The model runs on CPU\n",
      "Training epochs on cpu: 100%|██████████| 1/1 [22:14<00:00, 1334.17s/epoch, loss=1.03, prev_loss=nan]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu:   2%|▏         | 2.11k/102k [13:42<10:24:28, 2.65triple/s]"
     ]
    }
   ],
   "source": [
    "# trening\n",
    "tf = TriplesFactory.from_labeled_triples(np.array(triples))\n",
    "# ta dysproporcja jest po to, by szybko uzyskac jakikolwiek wynik\n",
    "#   - dla uzyskania sensownych wynikow warto zmienic split na np. standardowe 80-10-10\n",
    "training, testing, validation = tf.split([.8, .1, .1])\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    training=training,\n",
    "    testing=testing,\n",
    "    validation=validation,\n",
    "    model=TransE, # to najszybszy i najprostszy, ale i najgorszy model; pomysl o uzyciu TransH, TransR, RESCAL albo dowolnego innego\n",
    "    epochs=1 # to zdecydowanie za malo - 1 wystarczy do jakichkolwiek wynikow, 5 do dosc slabych, blizej 20 do sensownych\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bf67a-f449-4296-91e5-cadad5d4990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zgrubne oszacowanie jakosci wytrenowanego modelu\n",
    "pipeline_result.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37eaf5f3-8083-4284-8156-4f8326671050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja zwracajaca predykcje z modelu\n",
    "def predict_next_tracks(track_id: str, k: int) -> list[str]:\n",
    "    return predict_target(\n",
    "        model=pipeline_result.model, \n",
    "        triples_factory=tf,\n",
    "        relation='follows',\n",
    "        head=track_id,\n",
    "    ).df.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83eba4-2e1e-4bec-be87-8063ed71ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metryki do porownania rekomenderow - precision@k i recall@k\n",
    "def precision(prediction: list[str], actual_tracks: list[str]) -> float:\n",
    "    return len(set(prediction) & set(actual_tracks)) / len(prediction)\n",
    "\n",
    "def recall(prediction: list[str], actual_tracks: list[str]) -> float:\n",
    "    return len(set(prediction) & set(actual_tracks)) / len(actual_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cbf2e-e686-45d0-a8cd-5343132b2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocena wynikow\n",
    "\n",
    "# odsiewamy playlisty zbyt krotkie, by dac sensowne wyniki\n",
    "long_enough_test_playlists = [p for p in test_playlists if len(p) >= 10]\n",
    "# z kazdej playlisty, elementy od 0 do `cutoff_idx` wlacznie sa dane, na ich podstawie robimy predykcje\n",
    "# elementy od cutoff_idx+1 do konca powinnismy umiec przewidziec\n",
    "cutoff_idx = 4\n",
    "# ile elementow ma przewidziec nasz model\n",
    "k = 20\n",
    "precisions: list[float] = []\n",
    "recalls: list[float] = []\n",
    "\n",
    "for playlist in tqdm(long_enough_test_playlists):\n",
    "    prediction = predict_next_tracks(playlist[cutoff_idx], k)\n",
    "    precisions.append(precision(prediction, playlist[cutoff_idx+1:]))\n",
    "    recalls.append(recall(prediction, playlist[cutoff_idx+1:]))\n",
    "\n",
    "# histogram z wynikami\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(precisions, bins=20)\n",
    "ax1.set_title('Precision')\n",
    "ax2.hist(recalls, bins=20)\n",
    "ax2.set_title('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d83b9-de72-4fc0-8bfe-7acb6c66f62f",
   "metadata": {},
   "source": [
    "## Część 3. - porównanie różnych metod rekomendacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e87e7c-b9fb-430d-86cb-14433ddd4287",
   "metadata": {},
   "source": [
    "W części 2. zbudowaliśmy zbiór trójek relacji, wytrenowaliśmy jeden model, zaimplementowaliśmy (prostą) metodę generującą rekomendacje na podstawie predykcji modelu i w końcu zaimplementowaliśmy dwie metryki do porównania jakości tych rekomendacji.\n",
    "\n",
    "W części 3. Twoim zadaniem jest przetestować trzy różne podejścia do jednego z kroków:\n",
    "1. Porównaj trzy różne modele spośród dostępnych w bibliotece PyKeen: https://pykeen.readthedocs.io/en/stable/reference/models.html#classes\n",
    "   * jeden model translacyjny (np. TransE, TransH, TransR)\n",
    "   * jeden model faktoryzacyjny (np. RESCAL)\n",
    "   * jeden dowolny model niewybrany w poprzednich punktach\n",
    "2. Porównaj trzy metody budowania grafu wiedzy:\n",
    "   * graf zawierający relacje wszystkich czterech typów\n",
    "   * graf zawierający tylko relacje typu `follows` (czyli między kolejnymi utworami w playliście)\n",
    "   * graf zawierający relacje wybranych przez Ciebie dwóch lub trzech typów (czyli krok pośredni między powyższymi punktami)\n",
    "3. Porównaj trzy metody generowania rekomendacji na podstawie elementów zwróconych przez `predict_target()` (ta metoda zwraca m. in. score'y każdego z proponowanych elementów, co może okazać się pomocne):\n",
    "   * metoda opierająca się tylko na predykcji dla ostatniego znanego elementu w playliście\n",
    "   * dwie wymyślone przez Ciebie, bardziej zaawansowane metody\n",
    "  \n",
    "Niezależnie od tego, który z trzech powyższych scenariuszy wybierzesz - porównaj trzy wybrane przez Ciebie metody na podstawie histogramów metryk `precision@k` i `recall@k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e81534-66a5-4a81-a974-90b6496dffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [['follows'], ['authored_by', 'in_album'], ['follows', 'authored_by', 'in_album', 'authored']]\n",
    "\n",
    "for option in options:\n",
    "    triples = []\n",
    "    if 'authored_by' in option:\n",
    "        for track, artist in tqdm(tracks_to_artists):\n",
    "            triples.append((track, 'authored_by', artist))\n",
    "    if 'in_album' in option:\n",
    "        for track, album in tqdm(tracks_to_albums):\n",
    "            triples.append((track, 'in_album', album))\n",
    "    if 'authored' in option:\n",
    "        for album, artist in tqdm(albums_to_artists):\n",
    "            triples.append((artist, 'authored', album))\n",
    "    if 'follows' in option:\n",
    "        for playlist in tqdm(train_playlists):\n",
    "            for i in range(len(playlist)-1):\n",
    "                triples.append((playlist[i], 'follows', playlist[i+1]))\n",
    "    tf = TriplesFactory.from_labeled_triples(np.array(triples))\n",
    "    training, testing, validation = tf.split([.8, .1, .1])\n",
    "    pipeline_result = pipeline(\n",
    "        training=training,\n",
    "        testing=testing,\n",
    "        validation=validation,\n",
    "        model=TransE,\n",
    "        epochs=15\n",
    "    )\n",
    "    pipeline_result.plot_losses()\n",
    "    cutoff_idx = 4\n",
    "    k = 20\n",
    "    precisions: list[float] = []\n",
    "    recalls: list[float] = []\n",
    "\n",
    "    def predict_next_tracks(track_id: str, k: int) -> list[str]:\n",
    "        return predict_target(\n",
    "            model=pipeline_result.model,\n",
    "            query=track_id,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "    for playlist in tqdm(long_enough_test_playlists):\n",
    "        prediction = predict_next_tracks(playlist[cutoff_idx], k)\n",
    "        precisions.append(precision(prediction, playlist[cutoff_idx+1:]))\n",
    "        recalls.append(recall(prediction, playlist[cutoff_idx+1:]))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.hist(precisions, bins=20)\n",
    "    ax1.set_title('Precision')\n",
    "    ax2.hist(recalls, bins=20)\n",
    "    ax2.set_title('Recall')\n",
    "    ax2.set_title('Recall')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
